"""LLM-powered tools for code analysis and security scanning."""

import os
import sys
import json
import re
import time
from typing import Dict, List, Optional
from langchain.tools import tool
try:
    from langchain_google_genai import ChatGoogleGenerativeAI
except ImportError:
    from langchain_community.chat_models import ChatGoogleGenerativeAI

# Rate limiting: track last call time
_last_llm_call_time = 0
_min_delay_between_calls = 15  # 15 seconds between calls (Gemini free tier: 5 req/min = 12 sec min)
# Note: We use 15 sec here, but add additional delays in agent.py for file processing


@tool
def read_file_content(filepath: str) -> str:
    """
    Read the full content of a file from the repository.
    
    Args:
        filepath: Path to the file (can be relative to repo root or absolute)
        
    Returns:
        File content as string
    """
    try:
        repo_path = os.environ.get('DEVGUARD_REPO_PATH', '')
        
        # Handle relative paths
        if not os.path.isabs(filepath) and repo_path:
            filepath = os.path.join(repo_path, filepath)
        
        if not os.path.exists(filepath):
            error_msg = f"Error: File not found: {filepath}"
            if 'st' in globals() or 'streamlit' in sys.modules:
                try:
                    import streamlit as st
                    st.warning(f"⚠️ {error_msg}")
                except:
                    pass
            return error_msg
        
        with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read()
        return content
    except Exception as e:
        import sys
        error_msg = f"Error reading file {filepath}: {str(e)}"
        if 'st' in globals() or 'streamlit' in sys.modules:
            try:
                import streamlit as st
                st.error(f"❌ {error_msg}")
                with st.expander("File Read Error Details"):
                    import traceback
                    st.code(traceback.format_exc(), language="python")
            except:
                pass
        return error_msg


@tool
def analyze_code_with_llm(input_data: str) -> str:
    """
    Analyze code content using LLM to detect security risks and provide advice.
    Input can be:
    - File path (string) - will read file
    - Code content directly (string) - will analyze directly
    - JSON string with "filepath" and "file_content" keys
    
    Args:
        input_data: File path, code content, or JSON string
        
    Returns:
        JSON string with structured findings
    """
    import json
    
    # Parse input - check if it's a file path or direct code content
    filepath = "code_chunks"
    file_content = ""
    
    # Ensure input_data is a string, not a list
    if isinstance(input_data, list):
        # If it's a list, join it into a string
        file_content = "\n\n".join(str(item) for item in input_data)
        filepath = "filtered_code_chunks"
    elif isinstance(input_data, dict):
        # If it's a dict, convert to JSON string
        file_content = json.dumps(input_data)
        filepath = "code_chunks"
    else:
        # It should be a string
        input_str = str(input_data) if not isinstance(input_data, str) else input_data
        
        try:
            if input_str.startswith('{'):
                # JSON format
                data = json.loads(input_str)
                filepath = data.get("filepath", "code_chunks")
                file_content = data.get("file_content", "")
            elif os.path.exists(input_str):
                # File path - read the content
                filepath = input_str
                file_content = read_file_content.invoke({"filepath": filepath})
            else:
                # Direct code content (from filtered chunks)
                file_content = input_str
                filepath = "filtered_code_chunks"
        except Exception:
            # Fallback: treat as direct code content
            file_content = input_str
            filepath = "code_chunks"
    
    api_key = os.environ.get("GEMINI_API_KEY")
    if not api_key:
        return json.dumps({"error": "GEMINI_API_KEY not set"})
    
    # Initialize Gemini LLM for analysis
    llm = ChatGoogleGenerativeAI(
        model="gemini-3-flash-preview",  # Google Gemini model
        google_api_key=api_key,
        temperature=0.2,
    )
    
    # Create analysis prompt - ALL reasoning generated by LLM, no hardcoded rules
    # Check if this is filtered chunks or full file
    is_chunks = "chunk" in filepath.lower() or "--- CHUNK ---" in file_content
    
    if is_chunks:
        prompt_intro = f"""You are analyzing FILTERED RISKY CODE SECTIONS that were pre-identified as potentially containing security issues.

These code chunks have been filtered using pattern matching for security risks. Each chunk is from a different file, indicated by "File: <filename>" at the start of each chunk.

CRITICAL: Use the exact file name from the chunk header (e.g., "File: app.py" → use "app.py" as file_name).

Code Chunks:
```
{file_content[:8000]}
```
"""
    else:
        prompt_intro = f"""You are an expert security analyst with deep knowledge of code security, vulnerabilities, and best practices.

Analyze this file for security risks, vulnerabilities, and compliance issues:

File: {filepath}

Code Content:
```
{file_content[:8000]}
```
"""
    
    analysis_prompt = prompt_intro + """

You are an expert security analyst and AI agent with deep knowledge of code security, vulnerabilities, attack vectors, and remediation strategies.

YOUR AGENTIC MISSION - Act as an autonomous security expert:

1. **AUTONOMOUS ANALYSIS**: Examine the code chunks systematically, reasoning about each potential risk
2. **CONTEXT-AWARE DETECTION**: Consider the broader security implications, not just surface-level issues
3. **ATTACK VECTOR ANALYSIS**: Think like an attacker - how could this vulnerability be exploited?
4. **COMPREHENSIVE EXPLANATION**: Provide detailed, agent-like reasoning for each finding

FOR EACH SECURITY ISSUE YOU FIND, PROVIDE:

**1. DETAILED EXPLANATION** (`why_problem`):
   - Explain what the vulnerability is in detail
   - Describe the attack scenarios (step-by-step: how an attacker would exploit this)
   - Analyze the potential impact (what data/systems could be compromised)
   - Explain why this is a security risk (what makes it dangerous)
   - Discuss compliance implications (GDPR, PCI-DSS, HIPAA, OWASP Top 10)
   - Provide real-world examples of similar vulnerabilities that caused breaches

**2. SPECIFIC REMEDIATION STRATEGY** (`fix_suggestion`):
   - Provide step-by-step instructions on how to fix the issue
   - Include best practices and security standards to follow
   - Explain the correct secure implementation approach
   - Mention relevant security frameworks/guidelines (OWASP, CWE, NIST)

**3. DETAILED CODE CHANGES** (`what_to_change`):
   - Show the exact problematic code/configuration (copy it)
   - Provide the secure replacement code/configuration (with full context)
   - Explain why the new approach is secure
   - Include additional security measures to implement

**4. PRIORITY & SEVERITY ASSESSMENT**:
   - Assess severity (High/Medium/Low) based on:
     * Exploitability (how easy is it to exploit?)
     * Impact (what's at stake if exploited?)
     * Prevalence (how common is this vulnerability?)
   - Explain your severity reasoning

THINK DEEPLY ABOUT:
- Secrets and credentials (API keys, passwords, tokens) - where they're stored, how they're exposed, attack vectors
- Authentication and authorization vulnerabilities - broken access controls, privilege escalation
- Injection vulnerabilities (SQL, XSS, command injection, LDAP, NoSQL) - how they work, how to prevent them
- Insecure configurations - misconfigurations, default passwords, exposed services
- Data exposure risks - sensitive data in code, logs, responses, error messages
- Cryptographic weaknesses - weak encryption, improper key management, deprecated algorithms
- Code execution vulnerabilities - RCE, deserialization attacks, unsafe eval/exec
- Compliance violations - GDPR (data protection), PCI-DSS (payment data), HIPAA (health data)
- Supply chain risks - vulnerable dependencies, typosquatting
- API security - insecure endpoints, missing rate limiting, broken authentication

CRITICAL AGENT REQUIREMENTS:
- You MUST identify ALL security issues - be thorough and systematic
- Use your security expertise to reason deeply about WHY each issue matters
- Provide actionable, specific guidance - developers should know exactly what to do
- Think like both a defender (how to protect) and an attacker (how this could be exploited)
- Return ONLY valid JSON with findings array (even if empty, but analyze thoroughly)

EXAMINE THOROUGHLY FOR:
- Hardcoded secrets (API keys, passwords, tokens, certificates)
- SQL injection (unsafe string concatenation, missing parameterization)
- XSS vulnerabilities (unvalidated user input, unsafe DOM manipulation)
- Insecure configurations (open permissions, 0.0.0.0/0 in security groups, debug mode in production)
- Code execution vulnerabilities (eval with user input, unsafe deserialization)
- Authentication/authorization flaws (missing authentication, broken access controls)
- Sensitive data exposure (secrets in logs, error messages, API responses)
- Cryptographic weaknesses (weak keys, deprecated algorithms, improper usage)

OUTPUT FORMAT (JSON only, no other text):
{{
  "findings": [
    {{
      "file_name": "<EXACT file name from the chunk header, e.g. 'app.py' or 'aws_config.yml' - NOT 'filtered_code_chunks' or 'code_chunks'>",
      "line_number": <line number where issue appears in that specific file>,
      "risk_type": "<your assessment of risk type>",
      "severity": "<High|Medium|Low>",
      "description": "<brief 1-2 sentence summary of the security issue>",
      "fix_suggestion": "<COMPREHENSIVE step-by-step remediation strategy: explain how to fix it, what security frameworks/standards apply (OWASP, CWE, NIST), best practices to follow, and the correct secure implementation approach. Be detailed and actionable.>",
      "what_to_change": "<DETAILED code/configuration changes: show exact problematic code (with context), provide secure replacement code (with full context), explain why new approach is secure, include additional security measures. Use code blocks format.>",
      "why_problem": "<COMPREHENSIVE agent reasoning: explain what the vulnerability is in detail, describe step-by-step attack scenarios (how attacker exploits it), analyze potential impact (what data/systems compromised), explain why it's dangerous, discuss compliance implications (GDPR/PCI-DSS/HIPAA/OWASP), provide real-world breach examples if relevant. Be thorough and educational.>"
    }}
  ]
}}

CRITICAL OUTPUT REQUIREMENTS:
- **why_problem**: Must be a comprehensive, detailed explanation (300-500 words minimum). Explain attack vectors, impact, compliance issues, real-world examples.
- **fix_suggestion**: Must provide step-by-step remediation strategy with security frameworks, best practices, and implementation guidance (200-400 words).
- **what_to_change**: Must show exact code changes with before/after examples, context, and security explanations (150-300 words with code blocks).
- **description**: Brief summary (1-2 sentences).

IMPORTANT FILE NAMING:
- Look at each chunk's header: "File: <filename>" 
- Use that EXACT filename in "file_name" field (e.g., "app.py", "config.py", "aws_config.yml", "firebase.json")
- DO NOT use generic names like "filtered_code_chunks", "code_chunks", or "{filepath}"
- Each finding must have the correct file name from its corresponding chunk

Return your comprehensive analysis as JSON now:"""
    
    try:
        # Rate limiting: wait if needed to avoid quota errors
        global _last_llm_call_time
        current_time = time.time()
        time_since_last_call = current_time - _last_llm_call_time
        if time_since_last_call < _min_delay_between_calls:
            wait_time = _min_delay_between_calls - time_since_last_call
            time.sleep(wait_time)
        _last_llm_call_time = time.time()
        
        # Get LLM analysis
        response = llm.invoke(analysis_prompt)
        
        # Extract content from response - handle different response types
        output_text = None
        
        # Handle LangChain message objects - extract content from message parts
        if hasattr(response, 'content'):
            content = response.content
            
            # If content is a list of message parts (e.g., [{'type': 'text', 'text': '...'}])
            if isinstance(content, list):
                # Extract text from message parts
                text_parts = []
                for item in content:
                    if isinstance(item, dict):
                        # Handle dict format: {'type': 'text', 'text': '...'}
                        if 'text' in item:
                            text_parts.append(str(item['text']))
                        elif 'content' in item:
                            text_parts.append(str(item['content']))
                        else:
                            # If dict doesn't have text/content, convert to string
                            text_parts.append(str(item))
                    elif isinstance(item, str):
                        text_parts.append(item)
                    else:
                        text_parts.append(str(item))
                output_text = "".join(text_parts) if text_parts else None
            elif isinstance(content, str):
                output_text = content
            elif isinstance(content, dict):
                # Handle dict format: {'type': 'text', 'text': '...'}
                if 'text' in content:
                    output_text = str(content['text'])
                elif 'content' in content:
                    output_text = str(content['content'])
                else:
                    output_text = str(content)
            else:
                output_text = str(content) if content else None
        elif hasattr(response, 'text'):
            output_text = response.text
        elif isinstance(response, str):
            output_text = response
        elif isinstance(response, dict):
            # Handle dict format: {'type': 'text', 'text': '...'}
            if 'text' in response:
                output_text = str(response['text'])
            elif 'content' in response:
                output_text = str(response['content'])
            else:
                output_text = str(response)
        elif isinstance(response, list):
            # If response itself is a list, extract text from each item
            text_parts = []
            for item in response:
                if isinstance(item, dict):
                    if 'text' in item:
                        text_parts.append(str(item['text']))
                    elif 'content' in item:
                        text_parts.append(str(item['content']))
                    else:
                        text_parts.append(str(item))
                else:
                    text_parts.append(str(item))
            output_text = "".join(text_parts) if text_parts else None
        else:
            output_text = str(response) if response else None
        
        # Ensure output_text is a string
        if output_text is None:
            output_text = ""
        elif not isinstance(output_text, str):
            output_text = str(output_text)
        
        # Handle None or empty response
        if not output_text or output_text == "None":
            return json.dumps({
                "error": "LLM returned empty response",
                "filepath": filepath,
                "findings": []
            })
        
        # Extract JSON from response (LLM might add markdown formatting or extra text)
        # Try to find JSON in markdown code blocks first
        json_str = None
        
        # Pattern 1: Look for JSON in markdown code blocks (```json ... ``` or ``` ... ```)
        markdown_json_match = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', output_text, re.DOTALL)
        if markdown_json_match:
            json_str = markdown_json_match.group(1)
            try:
                findings_data = json.loads(json_str)
                return json.dumps(findings_data, indent=2)
            except json.JSONDecodeError:
                pass
        
        # Pattern 2: Look for JSON object in the response (most common)
        json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', output_text, re.DOTALL)
        if json_match:
            json_str = json_match.group(0)
            try:
                findings_data = json.loads(json_str)
                return json.dumps(findings_data, indent=2)
            except json.JSONDecodeError:
                # Try to extract just the inner JSON if there's extra text
                inner_json_match = re.search(r'\{\s*"findings"\s*:\s*\[.*?\]\s*\}', output_text, re.DOTALL)
                if inner_json_match:
                    try:
                        findings_data = json.loads(inner_json_match.group(0))
                        return json.dumps(findings_data, indent=2)
                    except json.JSONDecodeError:
                        pass
        
        # Pattern 3: Try parsing entire output as JSON (might work if LLM returned pure JSON)
        try:
            findings_data = json.loads(output_text.strip())
            return json.dumps(findings_data, indent=2)
        except json.JSONDecodeError:
            pass
        
        # Pattern 4: Try to extract and fix common JSON issues
        # Remove leading/trailing whitespace and try to find JSON structure
        cleaned_text = output_text.strip()
        
        # Look for "findings" array pattern
        findings_match = re.search(r'(\{\s*"findings"\s*:\s*\[.*?\]\s*\})', cleaned_text, re.DOTALL)
        if findings_match:
            try:
                findings_data = json.loads(findings_match.group(1))
                return json.dumps(findings_data, indent=2)
            except json.JSONDecodeError:
                pass
        
        # If all parsing fails, return error with debug info
        # Debug: Log what we received (first 1000 chars)
        debug_info = output_text[:1000] if len(output_text) > 1000 else output_text
        
        return json.dumps({
            "error": "Could not parse LLM response as JSON",
            "raw_output_preview": debug_info,
            "filepath": filepath,
            "findings": []
        })
    
    except Exception as llm_error:
        import sys
        error_str = str(llm_error)
        
        # Check if it's a quota/rate limit error
        if "429" in error_str or "RESOURCE_EXHAUSTED" in error_str or "quota" in error_str.lower():
            # Check if it's daily quota (20 requests/day) or rate limit (5 req/min)
            if "limit: 20" in error_str or "GenerateRequestsPerDay" in error_str:
                error_msg = (
                    "⚠️ **Daily quota exceeded!** Gemini free tier allows only 20 requests per day. "
                    "You've reached your daily limit. Please:\n"
                    "- Wait until tomorrow (quota resets daily)\n"
                    "- Upgrade to a paid Gemini API plan for higher limits\n"
                    "- Or use a different API key"
                )
            elif "limit: 5" in error_str or "per minute" in error_str.lower():
                error_msg = (
                    "⚠️ **Rate limit exceeded!** Gemini free tier allows 5 requests per minute. "
                    "Please wait a minute and try again."
                )
            else:
                error_msg = (
                    "⚠️ **Gemini API quota/rate limit exceeded.** "
                    "Free tier limits: 5 requests/minute and 20 requests/day. "
                    "Please wait or upgrade your API plan."
                )
            
            # Extract retry delay if available
            if "retry in" in error_str.lower():
                # re is already imported at top of file
                retry_match = re.search(r'retry in ([\d.]+)s', error_str.lower())
                if retry_match:
                    wait_seconds = float(retry_match.group(1))
                    error_msg += f"\n\nSuggested wait time: {wait_seconds:.0f} seconds."
        else:
            error_msg = f"LLM API call failed: {error_str}"
        
        if 'st' in globals() or 'streamlit' in sys.modules:
            try:
                import streamlit as st
                st.warning(f"⚠️ {error_msg}")
                with st.expander("LLM API Error Details"):
                    import traceback
                    st.code(traceback.format_exc(), language="python")
            except:
                pass
        return json.dumps({
            "error": error_msg,
            "filepath": filepath,
            "findings": []
        })


def parse_llm_findings(llm_output: str) -> List[Dict]:
    """
    Parse LLM output into structured findings list.
    
    Args:
        llm_output: JSON string from analyze_code_with_llm (can be None)
        
    Returns:
        List of finding dictionaries
    """
    # Handle None or empty input
    if not llm_output or llm_output is None:
        return []
    
    try:
        data = json.loads(llm_output)
        if "findings" in data:
            return data["findings"]
        elif isinstance(data, list):
            return data
        else:
            return []
    except (json.JSONDecodeError, TypeError) as e:
        # Return empty list if parsing fails
        return []

